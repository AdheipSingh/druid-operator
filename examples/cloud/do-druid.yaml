apiVersion: "druid.apache.org/v1alpha1"
kind: "Druid"
metadata:
  name: do-cluster
spec:
  image: apache/druid:0.21.1
  startScript: /druid.sh
  podLabels:
    environment: do
    release: alpha
  podAnnotations:
    cloud: do
  securityContext:
    fsGroup: 1000
    runAsUser: 1000
    runAsGroup: 1000
  services:
    - spec:
        type: ClusterIP
        clusterIP: None
  commonConfigMountPath: "/opt/druid/conf/druid/cluster/_common"
  jvm.options: |-
    -server
    -XX:MaxDirectMemorySize=10240g
    -Duser.timezone=UTC
    -Dfile.encoding=UTF-8
    -Dlog4j.debug
    -Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager
    -Djava.io.tmpdir=/druid/data
  log4j.config: |-
    <?xml version="1.0" encoding="UTF-8" ?>
    <Configuration status="WARN">
        <Appenders>
            <Console name="Console" target="SYSTEM_OUT">
                <PatternLayout pattern="%d{ISO8601} %p [%t] %c - %m%n"/>
            </Console>
        </Appenders>
        <Loggers>
            <Root level="info">
                <AppenderRef ref="Console"/>
            </Root>
        </Loggers>
    </Configuration>
  volumeMounts:
    - mountPath: /druid/data
      name: data-volume
  volumes:
    - name: data-volume
      emptyDir: {}
  common.runtime.properties: |

    # metadata storage
    druid.metadata.storage.type=derby
    druid.metadata.storage.connector.connectURI=jdbc:derby://localhost:1527/druid/data/derbydb/metadata.db;create=true
    druid.metadata.storage.connector.host=localhost
    druid.metadata.storage.connector.port=1527
    druid.metadata.storage.connector.createTables=true

    # deep storage
    druid.storage.type=s3
    druid.storage.bucket=bucketname
    druid.storage.baseKey=druid/segments
    druid.s3.endpoint.url=fra1.digitaloceanspaces.com:443
    druid.s3.endpoint.signingRegion=fra1
    druid.s3.accessKey=VBACCESSKEYPJ
    druid.s3.secretKey=mSeCrEtKeyyY

    # druid-k8s-extension-configuration
    druid.zk.service.enabled=false
    druid.serverview.type=http
    druid.coordinator.loadqueuepeon.type=http
    druid.indexer.runner.type=httpRemote
    druid.discovery.type=k8s
    druid.discovery.k8s.clusterIdentifier=us-west-do-druid

    #
    # Extensions
    #
    druid.extensions.loadList=["druid-kafka-indexing-service","druid-s3-extensions","druid-kubernetes-extensions"]
    #
    # Service discovery
    #
    druid.selectors.indexing.serviceName=druid/overlord
    druid.selectors.coordinator.serviceName=druid/coordinator
    druid.lookup.enableLookupSyncOnStartup=false
  env:
    - name: POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    - name: POD_NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace
  nodes:
    brokers:
      kind: Deployment
      nodeType: "broker"
      druid.port: 8080
      nodeConfigMountPath: "/opt/druid/conf/druid/cluster/query/broker"
      replicas: 1
      runtime.properties: |
        druid.service=druid/broker
        # HTTP server threads
        druid.broker.http.numConnections=1
        druid.server.http.numThreads=1
        # Processing threads and buffers
        druid.processing.buffer.sizeBytes=1
        druid.processing.numMergeBuffers=1
        druid.processing.numThreads=1
        druid.sql.enable=true

    coordinators:
      kind: Deployment
      nodeType: "coordinator"
      druid.port: 8080
      nodeConfigMountPath: "/opt/druid/conf/druid/cluster/master/coordinator-overlord"
      replicas: 1
      runtime.properties: |
        druid.service=druid/coordinator
        # HTTP server threads
        druid.coordinator.startDelay=PT30S
        druid.coordinator.period=PT30S

        # Configure this coordinator to also run as Overlord
        druid.coordinator.asOverlord.enabled=true
        druid.coordinator.asOverlord.overlordService=druid/overlord
        druid.indexer.queue.startDelay=PT30S
      extra.jvm.options: |-
        -Xmx512M
        -Xms512M

    historicals:
      nodeType: "historical"
      druid.port: 8080
      nodeConfigMountPath: "/opt/druid/conf/druid/cluster/data/historical"
      replicas: 1
      volumeClaimTemplates:
        -
          metadata:
            name: historical-volume
          spec:
            accessModes:
              - ReadWriteOnce
            resources:
              requests:
                storage: 50Gi
            storageClassName: do-block-storage
      volumeMounts:
        -
          mountPath: var/druid
          name: historical-volume
      runtime.properties: |
        druid.service=druid/historical
        druid.server.http.numThreads=1
        druid.processing.buffer.sizeBytes=400000
        druid.processing.tmpDir=var/druid/processing
        druid.processing.numMergeBuffers=1
        druid.processing.numThreads=1

        druid.segmentCache.locations=[{"path":"var/druid/segment-cache","maxSize":40000000}]
        druid.server.maxSize=40000000000
      extra.jvm.options: |-
        -Xmx512M
        -Xms512M

    middlemanagers:
      druid.port: 8080
      kind: StatefulSet
      nodeType: middleManager
      nodeConfigMountPath: /opt/druid/conf/druid/cluster/data/middleManager
      podDisruptionBudgetSpec:
        maxUnavailable: 1
      replicas: 1
      runtime.properties: |
          druid.service=druid/middleManager
          druid.worker.capacity=4
          druid.indexer.runner.javaOpts=-server -Xms1g -Xmx1g -XX:MaxDirectMemorySize=1g -Duser.timezone=UTC -Dfile.encoding=UTF-8 -XX:+ExitOnOutOfMemoryError -Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager
          druid.indexer.task.baseTaskDir=var/druid/task

          # HTTP server threads    
          druid.server.http.numThreads=60
          # Processing threads and buffers on Peons
          druid.indexer.fork.property.druid.processing.numMergeBuffers=2
          druid.indexer.fork.property.druid.processing.buffer.sizeBytes=100MiB
          druid.indexer.fork.property.druid.processing.numThreads=1
      volumeClaimTemplates:
        -
          metadata:
            name: data-volume
          spec:
            accessModes:
              - ReadWriteOnce
            resources:
              requests:
                storage: 15Gi
            storageClassName: do-block-storage
      volumeMounts:
        -
          mountPath: var/druid
          name: data-volume

    routers:
      nodeType: "router"
      kind: Deployment
      druid.port: 8888
      nodeConfigMountPath: "/opt/druid/conf/druid/cluster/query/router"
      replicas: 1
      runtime.properties: |
        druid.service=druid/router
        # HTTP proxy
        druid.router.http.numConnections=10
        druid.router.http.readTimeout=PT5M
        druid.router.http.numMaxThreads=10
        druid.server.http.numThreads=10

        # Service discovery
        druid.router.defaultBrokerServiceName=druid/broker
        druid.router.coordinatorServiceName=druid/coordinator
        # Management proxy to coordinator / overlord: required for unified web console.
        druid.router.managementProxy.enabled=true       
      extra.jvm.options: |-
        -Xmx512M
        -Xms512M
